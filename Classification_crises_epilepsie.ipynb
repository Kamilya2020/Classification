{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kamilya2020/Classification/blob/main/Classification_crises_epilepsie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZy8Y65tsUgH"
      },
      "source": [
        "**Importation des biblioth√®ques**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPUP0ixTT8aQ",
        "outputId": "f0d22b42-0627-4b7a-81ea-45a2ffed1983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n",
            "1.21.6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Conv2D, MaxPooling2D,Flatten,BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import regularizers\n",
        "import numpy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "\n",
        "numpy.random.seed(2)\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9N-lgkzsu3B"
      },
      "source": [
        "**Importation de la base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "bfwTiXMuXDva",
        "outputId": "4c260dbf-66c6-4083-df84-9f3c0e844eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13762, 7)\n",
            "(13762, 7)\n",
            "(13762, 7, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-18223b49659f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#print(x_test.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdataset_output\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./y2_e2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mdataset_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdataset_output_new\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdataset_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './y2_e2.txt'"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset_e1= pd.read_csv(\"./stand_norm_e1.txt\",delimiter=\" \",header=None)\n",
        "dataset_e1=numpy.array(dataset_e1,float)\n",
        "\n",
        "X_e1 = dataset_e1[:,(0,1,2,3,4,5,6)]\n",
        "#m=np.max(X_e1)\n",
        "print(X_e1.shape)\n",
        "dataset_e2= pd.read_csv(\"./stand_norm_e2.txt\",delimiter=\" \",header=None)\n",
        "dataset_e2=numpy.array(dataset_e2,float)\n",
        "\n",
        "X_e2 = dataset_e2[:,(0,1,2,3,4,5,6)]\n",
        "#n=np.max(X_e2)\n",
        "#X_e2=X_e2/n\n",
        "print(X_e2.shape)\n",
        "'''normalizing data with MinMaxScaler'''\n",
        "#scaler = preprocessing.MinMaxScaler()\n",
        "#X_e1_scaled = scaler.fit_transform(X_e1)\n",
        "#X_e2_scaled = scaler.fit_transform(X_e2)\n",
        "\n",
        "\n",
        "x_train_test=np.zeros((X_e1.shape[0],X_e1.shape[1],2))\n",
        "x_train_test[:,:,0]=X_e1\n",
        "x_train_test[:,:,1]=X_e2\n",
        "print(x_train_test.shape)\n",
        "\n",
        "\n",
        "#print(x_test.shape)\n",
        "dataset_output= pd.read_csv(\"./y2_e2.txt\",delimiter=\" \",header=None)\n",
        "dataset_output=numpy.array(dataset_output,float)\n",
        "dataset_output_new= dataset_output.reshape(dataset_output.shape[0], )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vb2k4ttC81T",
        "outputId": "12c2cdee-27e4-47cd-dc1f-78756bd83665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.38244  -0.480229]\n",
            "  [ 0.505189  1.716631]\n",
            "  [ 0.217562  0.354079]\n",
            "  ...\n",
            "  [-0.455253 -0.460837]\n",
            "  [-0.360255 -0.457283]\n",
            "  [-0.267133 -0.267133]]\n",
            "\n",
            " [[-0.424772 -0.494075]\n",
            "  [ 0.023558  1.887786]\n",
            "  [ 0.183694  0.643721]\n",
            "  ...\n",
            "  [-0.465147 -0.384227]\n",
            "  [-0.399712 -0.476274]\n",
            "  [-0.308448 -0.308448]]\n",
            "\n",
            " [[ 0.781768 -0.275606]\n",
            "  [ 0.236212  1.479074]\n",
            "  [ 0.192097  1.047914]\n",
            "  ...\n",
            "  [ 0.304319 -0.607677]\n",
            "  [ 0.812264 -0.229191]\n",
            "  [-0.349159 -0.349159]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.382491 -0.482518]\n",
            "  [-0.24978  -0.280554]\n",
            "  [-0.255483 -0.530712]\n",
            "  ...\n",
            "  [-0.377275 -0.428467]\n",
            "  [-0.350391 -0.43643 ]\n",
            "  [ 1.227973  1.227973]]\n",
            "\n",
            " [[-0.252338 -0.057465]\n",
            "  [-0.192436 -0.041017]\n",
            "  [-0.156271  0.648364]\n",
            "  ...\n",
            "  [-0.039089  0.673583]\n",
            "  [-0.25583  -0.194975]\n",
            "  [ 1.250906  1.250906]]\n",
            "\n",
            " [[-0.397746 -0.459311]\n",
            "  [-0.248198 -0.354015]\n",
            "  [-0.268403 -0.413529]\n",
            "  ...\n",
            "  [-0.343039 -0.303634]\n",
            "  [-0.370069 -0.430366]\n",
            "  [ 1.274114  1.274114]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCWpugis3xo"
      },
      "source": [
        "**R√©partition entre l'apprentissage et test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PnvFwYkYNTU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train_test, dataset_output_new, test_size=0.10, random_state=45)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new=x_train\n",
        "x_test_new=x_test"
      ],
      "metadata": {
        "id": "BlioLus1Mlru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiJy0EoltJ1L"
      },
      "source": [
        "**Pr√©paration des sorties du mod√®le**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xvAiyJEYjYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebad5460-e75d-4cf2-a26f-39a59571ab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12385,)\n",
            "(12385, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(y_train.shape)\n",
        "\n",
        "Y_train_new = tensorflow.keras.utils.to_categorical( y_train)\n",
        "Y_test_new = tensorflow.keras.utils.to_categorical(y_test)\n",
        "print(Y_train_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbU0MxfC4eJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e531681-6d8e-4517-fe5a-99fe9c12ae3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 1. ... 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKmwmgbutObe"
      },
      "source": [
        "**Pr√©paration des entr√©es du mod√®le**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGWS5nbAYpHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548d7709-9aaa-4b43-cd43-184aab88fa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12385, 7, 2)\n",
            "(12385, 7, 2)\n"
          ]
        }
      ],
      "source": [
        "print (x_train_new.shape)\n",
        "x_train_new=x_train_new.reshape(x_train_new.shape[0],  x_train_new.shape[1], 2)\n",
        "x_test_new = x_test_new.reshape(x_test_new.shape[0], x_test_new.shape[1],2 )\n",
        "print (x_train_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcfBrjptSH0"
      },
      "source": [
        "**Construction du mod√®le**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1WdZkYqYtlb"
      },
      "outputs": [],
      "source": [
        "from pandas.core.arrays import categorical\n",
        "def objective(trial):\n",
        "  #from tensorflow.keras.layers import Dense, Dropout,Conv1D, MaxPooling1D,Flatten,BatchNormalization\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]),kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]),input_shape=input_shape,data_format='channels_first',activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"])))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]), kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]), activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"],padding='same')))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]), kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]), activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"],padding='same')))\n",
        "  model.add(MaxPooling1D(pool_size=2,strides=True))\n",
        "  model.add(Dropout(0.4))#0.3\n",
        "\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]), kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]), activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"],padding='same')))\n",
        "  model.add(MaxPooling1D(pool_size=2,strides=True))\n",
        "  model.add(Dropout(0.4))#0.3\n",
        "\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]), kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]), activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"],padding='same')))\n",
        "  model.add(MaxPooling1D(pool_size=2,strides=True))\n",
        "  model.add(Dropout(0.4))#0.3\n",
        "\n",
        "\n",
        "  model.add(Conv1D(filters=trial.suggest-categorical(\"filters\",[32,64,128,256,512]), kernel_size=trial.suggest_categorical(\"kernel_size\",[1,3]), activation=trial.suggest_categorical(\"activation\",[\"relu\",\"sigmoid\",\"tanh\"],padding='same')))\n",
        "  model.add(MaxPooling1D(pool_size=2,strides=None))\n",
        "  model.add(Dropout(0.4))#0.4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.add(Flatten()) # Flatten is the input layer of the Fully Connected\n",
        "\n",
        "\n",
        "  model.add(Dense(200, activation='relu')) # 100 \n",
        "  lr=trial.suggest_float(\"lr\",1e-5,1e-1,log=True)\n",
        "  model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "  history =model.fit(x_train_new, Y_train_new, epochs=200, batch_size=1024, verbose=1, validation_split=0.15, callbacks=[checkpointer])\n",
        "  score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))#0.4\n",
        "\n",
        "  model.add(Dense(100, activation='relu')) # 100 \n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))#0.4\n",
        "\n",
        "#model.add(Dense(2, activation='softmax'))\n",
        "  model.add(Dense(3, activation='softmax',kernel_regularizer=regularizers.l2(0.001)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet optuna\n",
        "import optuna \n",
        "from optuna.visualization.plot_contour\n",
        "from optuna.visualization.plot_edf\n",
        "from optuna.visualization.plot_intermediate_values\n",
        "optuna.visualization.plot_optimization_history\n",
        "study=optuna.create_study(sampler=optuna.samplers.RandomSampler(),direction=\"maximize\")\n",
        "study.optimize(objective,n_trials=10)\n",
        "print(\"Number of finished trials:{}\".format(len))"
      ],
      "metadata": {
        "id": "7yE366qgQHXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWtyc6X9nFHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d891f2-c40d-44f2-a272-cc7f405856ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 7, 32)             224       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 7, 32)            128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 7, 64)             6208      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 7, 64)            256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 7, 128)            24704     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 6, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6, 128)            0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 6, 256)            98560     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 5, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 256)            0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 5, 512)            393728    \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 4, 512)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 512)            0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 4, 1024)           1573888   \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 2, 1024)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 2, 1024)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               409800    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,529,099\n",
            "Trainable params: 2,528,307\n",
            "Non-trainable params: 792\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ],
      "metadata": {
        "id": "uEvhtssEHCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H0zXxANtW_O"
      },
      "source": [
        "**Effectuer l'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAy_wUN3Y2TX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24cdcb9-e200-4842-d5e9-ff2b63cc7200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.5431 - accuracy: 0.3920 - precision: 0.4033 - recall: 0.2887\n",
            "Epoch 1: val_loss improved from inf to 1.05065, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 19s 221ms/step - loss: 1.5431 - accuracy: 0.3920 - precision: 0.4033 - recall: 0.2887 - val_loss: 1.0507 - val_accuracy: 0.6028 - val_precision: 0.4795 - val_recall: 0.0188\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2291 - accuracy: 0.4930 - precision: 0.5387 - recall: 0.3281\n",
            "Epoch 2: val_loss improved from 1.05065 to 1.00456, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.2291 - accuracy: 0.4930 - precision: 0.5387 - recall: 0.3281 - val_loss: 1.0046 - val_accuracy: 0.6098 - val_precision: 0.6368 - val_recall: 0.0651\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0559 - accuracy: 0.6226 - precision: 0.6737 - recall: 0.4974\n",
            "Epoch 3: val_loss improved from 1.00456 to 0.94590, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.0559 - accuracy: 0.6226 - precision: 0.6737 - recall: 0.4974 - val_loss: 0.9459 - val_accuracy: 0.5963 - val_precision: 0.6645 - val_recall: 0.4456\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.7008 - precision: 0.7361 - recall: 0.6190\n",
            "Epoch 4: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.9359 - accuracy: 0.7008 - precision: 0.7361 - recall: 0.6190 - val_loss: 0.9867 - val_accuracy: 0.5226 - val_precision: 0.5528 - val_recall: 0.5043\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8534 - accuracy: 0.7274 - precision: 0.7572 - recall: 0.6613\n",
            "Epoch 5: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.8534 - accuracy: 0.7274 - precision: 0.7572 - recall: 0.6613 - val_loss: 1.0542 - val_accuracy: 0.5145 - val_precision: 0.5152 - val_recall: 0.5113\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8007 - accuracy: 0.7501 - precision: 0.7713 - recall: 0.6948\n",
            "Epoch 6: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.8007 - accuracy: 0.7501 - precision: 0.7713 - recall: 0.6948 - val_loss: 1.0745 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7398 - accuracy: 0.7543 - precision: 0.7765 - recall: 0.7093\n",
            "Epoch 7: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.7398 - accuracy: 0.7543 - precision: 0.7765 - recall: 0.7093 - val_loss: 1.0736 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.7706 - precision: 0.7848 - recall: 0.7326\n",
            "Epoch 8: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.7013 - accuracy: 0.7706 - precision: 0.7848 - recall: 0.7326 - val_loss: 1.0870 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6721 - accuracy: 0.7761 - precision: 0.7920 - recall: 0.7436\n",
            "Epoch 9: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.6721 - accuracy: 0.7761 - precision: 0.7920 - recall: 0.7436 - val_loss: 1.0723 - val_accuracy: 0.5140 - val_precision: 0.5137 - val_recall: 0.5135\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.7769 - precision: 0.7924 - recall: 0.7475\n",
            "Epoch 10: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.6472 - accuracy: 0.7769 - precision: 0.7924 - recall: 0.7475 - val_loss: 1.0391 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7804 - precision: 0.7955 - recall: 0.7538\n",
            "Epoch 11: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.6281 - accuracy: 0.7804 - precision: 0.7955 - recall: 0.7538 - val_loss: 1.0505 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6113 - accuracy: 0.7859 - precision: 0.7979 - recall: 0.7604\n",
            "Epoch 12: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.6113 - accuracy: 0.7859 - precision: 0.7979 - recall: 0.7604 - val_loss: 1.0505 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.7901 - precision: 0.8035 - recall: 0.7678\n",
            "Epoch 13: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.5791 - accuracy: 0.7901 - precision: 0.8035 - recall: 0.7678 - val_loss: 1.0538 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.7923 - precision: 0.8037 - recall: 0.7727\n",
            "Epoch 14: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.5721 - accuracy: 0.7923 - precision: 0.8037 - recall: 0.7727 - val_loss: 1.0724 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.7968 - precision: 0.8088 - recall: 0.7798\n",
            "Epoch 15: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.5404 - accuracy: 0.7968 - precision: 0.8088 - recall: 0.7798 - val_loss: 1.0517 - val_accuracy: 0.5135 - val_precision: 0.5135 - val_recall: 0.5135\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.7985 - precision: 0.8097 - recall: 0.7807\n",
            "Epoch 16: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.5417 - accuracy: 0.7985 - precision: 0.8097 - recall: 0.7807 - val_loss: 1.0674 - val_accuracy: 0.5140 - val_precision: 0.5137 - val_recall: 0.5135\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.8032 - precision: 0.8117 - recall: 0.7884\n",
            "Epoch 17: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.5234 - accuracy: 0.8032 - precision: 0.8117 - recall: 0.7884 - val_loss: 1.0618 - val_accuracy: 0.5140 - val_precision: 0.5143 - val_recall: 0.5135\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4929 - accuracy: 0.8129 - precision: 0.8232 - recall: 0.7971\n",
            "Epoch 18: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4929 - accuracy: 0.8129 - precision: 0.8232 - recall: 0.7971 - val_loss: 1.0540 - val_accuracy: 0.5178 - val_precision: 0.5180 - val_recall: 0.5178\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8124 - precision: 0.8199 - recall: 0.7977\n",
            "Epoch 19: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4947 - accuracy: 0.8124 - precision: 0.8199 - recall: 0.7977 - val_loss: 1.0912 - val_accuracy: 0.5178 - val_precision: 0.5178 - val_recall: 0.5172\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.8142 - precision: 0.8217 - recall: 0.8017\n",
            "Epoch 20: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4891 - accuracy: 0.8142 - precision: 0.8217 - recall: 0.8017 - val_loss: 1.0874 - val_accuracy: 0.5194 - val_precision: 0.5192 - val_recall: 0.5178\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.8195 - precision: 0.8272 - recall: 0.8078\n",
            "Epoch 21: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4777 - accuracy: 0.8195 - precision: 0.8272 - recall: 0.8078 - val_loss: 1.0396 - val_accuracy: 0.5242 - val_precision: 0.5240 - val_recall: 0.5231\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.8159 - precision: 0.8236 - recall: 0.8035\n",
            "Epoch 22: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4637 - accuracy: 0.8159 - precision: 0.8236 - recall: 0.8035 - val_loss: 1.0651 - val_accuracy: 0.5264 - val_precision: 0.5259 - val_recall: 0.5253\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8168 - precision: 0.8233 - recall: 0.8063\n",
            "Epoch 23: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4631 - accuracy: 0.8168 - precision: 0.8233 - recall: 0.8063 - val_loss: 1.0744 - val_accuracy: 0.5285 - val_precision: 0.5291 - val_recall: 0.5280\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.8196 - precision: 0.8253 - recall: 0.8077\n",
            "Epoch 24: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4466 - accuracy: 0.8196 - precision: 0.8253 - recall: 0.8077 - val_loss: 1.0838 - val_accuracy: 0.5323 - val_precision: 0.5324 - val_recall: 0.5301\n",
            "Epoch 25/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.4412 - accuracy: 0.8247 - precision: 0.8309 - recall: 0.8150\n",
            "Epoch 25: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4400 - accuracy: 0.8246 - precision: 0.8309 - recall: 0.8151 - val_loss: 0.9790 - val_accuracy: 0.5608 - val_precision: 0.5616 - val_recall: 0.5597\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.8259 - precision: 0.8319 - recall: 0.8183\n",
            "Epoch 26: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4261 - accuracy: 0.8259 - precision: 0.8319 - recall: 0.8183 - val_loss: 0.9995 - val_accuracy: 0.5544 - val_precision: 0.5539 - val_recall: 0.5527\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8306 - precision: 0.8356 - recall: 0.8226\n",
            "Epoch 27: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4183 - accuracy: 0.8306 - precision: 0.8356 - recall: 0.8226 - val_loss: 1.0041 - val_accuracy: 0.5646 - val_precision: 0.5661 - val_recall: 0.5646\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.8282 - precision: 0.8329 - recall: 0.8198\n",
            "Epoch 28: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.4155 - accuracy: 0.8282 - precision: 0.8329 - recall: 0.8198 - val_loss: 1.0361 - val_accuracy: 0.5576 - val_precision: 0.5589 - val_recall: 0.5565\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.8385 - precision: 0.8422 - recall: 0.8319\n",
            "Epoch 29: val_loss did not improve from 0.94590\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4011 - accuracy: 0.8385 - precision: 0.8422 - recall: 0.8319 - val_loss: 1.0590 - val_accuracy: 0.5554 - val_precision: 0.5564 - val_recall: 0.5549\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8313 - precision: 0.8355 - recall: 0.8257\n",
            "Epoch 30: val_loss improved from 0.94590 to 0.92666, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4012 - accuracy: 0.8313 - precision: 0.8355 - recall: 0.8257 - val_loss: 0.9267 - val_accuracy: 0.5813 - val_precision: 0.5815 - val_recall: 0.5797\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8366 - precision: 0.8408 - recall: 0.8291\n",
            "Epoch 31: val_loss did not improve from 0.92666\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.4003 - accuracy: 0.8366 - precision: 0.8408 - recall: 0.8291 - val_loss: 0.9857 - val_accuracy: 0.5813 - val_precision: 0.5814 - val_recall: 0.5802\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.8420 - precision: 0.8459 - recall: 0.8360\n",
            "Epoch 32: val_loss improved from 0.92666 to 0.92394, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3856 - accuracy: 0.8420 - precision: 0.8459 - recall: 0.8360 - val_loss: 0.9239 - val_accuracy: 0.5818 - val_precision: 0.5832 - val_recall: 0.5813\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8401 - precision: 0.8449 - recall: 0.8347\n",
            "Epoch 33: val_loss did not improve from 0.92394\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3890 - accuracy: 0.8401 - precision: 0.8449 - recall: 0.8347 - val_loss: 0.9435 - val_accuracy: 0.5915 - val_precision: 0.5914 - val_recall: 0.5904\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.8424 - precision: 0.8457 - recall: 0.8376\n",
            "Epoch 34: val_loss did not improve from 0.92394\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3774 - accuracy: 0.8424 - precision: 0.8457 - recall: 0.8376 - val_loss: 0.9377 - val_accuracy: 0.6125 - val_precision: 0.6129 - val_recall: 0.6109\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3677 - accuracy: 0.8452 - precision: 0.8495 - recall: 0.8405\n",
            "Epoch 35: val_loss improved from 0.92394 to 0.82982, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3677 - accuracy: 0.8452 - precision: 0.8495 - recall: 0.8405 - val_loss: 0.8298 - val_accuracy: 0.6017 - val_precision: 0.6015 - val_recall: 0.5996\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8449 - precision: 0.8476 - recall: 0.8404\n",
            "Epoch 36: val_loss did not improve from 0.82982\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3808 - accuracy: 0.8449 - precision: 0.8476 - recall: 0.8404 - val_loss: 0.9263 - val_accuracy: 0.6033 - val_precision: 0.6037 - val_recall: 0.6033\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8463 - precision: 0.8481 - recall: 0.8409\n",
            "Epoch 37: val_loss improved from 0.82982 to 0.82724, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 70ms/step - loss: 0.3770 - accuracy: 0.8463 - precision: 0.8481 - recall: 0.8409 - val_loss: 0.8272 - val_accuracy: 0.6173 - val_precision: 0.6185 - val_recall: 0.6168\n",
            "Epoch 38/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.3839 - accuracy: 0.8425 - precision: 0.8462 - recall: 0.8385\n",
            "Epoch 38: val_loss improved from 0.82724 to 0.66789, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3850 - accuracy: 0.8427 - precision: 0.8464 - recall: 0.8387 - val_loss: 0.6679 - val_accuracy: 0.6760 - val_precision: 0.6759 - val_recall: 0.6722\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8510 - precision: 0.8539 - recall: 0.8475\n",
            "Epoch 39: val_loss did not improve from 0.66789\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3682 - accuracy: 0.8510 - precision: 0.8539 - recall: 0.8475 - val_loss: 0.7714 - val_accuracy: 0.6685 - val_precision: 0.6677 - val_recall: 0.6642\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8511 - precision: 0.8536 - recall: 0.8460\n",
            "Epoch 40: val_loss improved from 0.66789 to 0.62646, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3620 - accuracy: 0.8511 - precision: 0.8536 - recall: 0.8460 - val_loss: 0.6265 - val_accuracy: 0.7271 - val_precision: 0.7284 - val_recall: 0.7244\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.8514 - precision: 0.8550 - recall: 0.8472\n",
            "Epoch 41: val_loss did not improve from 0.62646\n",
            "11/11 [==============================] - 1s 45ms/step - loss: 0.3558 - accuracy: 0.8514 - precision: 0.8550 - recall: 0.8472 - val_loss: 0.6892 - val_accuracy: 0.6857 - val_precision: 0.6854 - val_recall: 0.6825\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8559 - precision: 0.8591 - recall: 0.8524\n",
            "Epoch 42: val_loss did not improve from 0.62646\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.3491 - accuracy: 0.8559 - precision: 0.8591 - recall: 0.8524 - val_loss: 0.6464 - val_accuracy: 0.6991 - val_precision: 0.7012 - val_recall: 0.6986\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8559 - precision: 0.8589 - recall: 0.8510\n",
            "Epoch 43: val_loss did not improve from 0.62646\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3413 - accuracy: 0.8559 - precision: 0.8589 - recall: 0.8510 - val_loss: 0.6598 - val_accuracy: 0.7244 - val_precision: 0.7252 - val_recall: 0.7228\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.8578 - precision: 0.8603 - recall: 0.8539\n",
            "Epoch 44: val_loss did not improve from 0.62646\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3373 - accuracy: 0.8578 - precision: 0.8603 - recall: 0.8539 - val_loss: 0.6517 - val_accuracy: 0.6964 - val_precision: 0.6965 - val_recall: 0.6943\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8542 - precision: 0.8570 - recall: 0.8502\n",
            "Epoch 45: val_loss improved from 0.62646 to 0.54208, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3389 - accuracy: 0.8542 - precision: 0.8570 - recall: 0.8502 - val_loss: 0.5421 - val_accuracy: 0.7524 - val_precision: 0.7534 - val_recall: 0.7513\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.8639 - precision: 0.8666 - recall: 0.8610\n",
            "Epoch 46: val_loss did not improve from 0.54208\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.3346 - accuracy: 0.8639 - precision: 0.8666 - recall: 0.8610 - val_loss: 0.5655 - val_accuracy: 0.7438 - val_precision: 0.7454 - val_recall: 0.7422\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.8651 - precision: 0.8671 - recall: 0.8612\n",
            "Epoch 47: val_loss did not improve from 0.54208\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 0.3279 - accuracy: 0.8651 - precision: 0.8671 - recall: 0.8612 - val_loss: 0.5928 - val_accuracy: 0.7336 - val_precision: 0.7339 - val_recall: 0.7320\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8637 - precision: 0.8667 - recall: 0.8610\n",
            "Epoch 48: val_loss did not improve from 0.54208\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3270 - accuracy: 0.8637 - precision: 0.8667 - recall: 0.8610 - val_loss: 0.6167 - val_accuracy: 0.7341 - val_precision: 0.7353 - val_recall: 0.7325\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8617 - precision: 0.8636 - recall: 0.8579\n",
            "Epoch 49: val_loss improved from 0.54208 to 0.50707, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3318 - accuracy: 0.8617 - precision: 0.8636 - recall: 0.8579 - val_loss: 0.5071 - val_accuracy: 0.7686 - val_precision: 0.7686 - val_recall: 0.7670\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8652 - precision: 0.8673 - recall: 0.8623\n",
            "Epoch 50: val_loss improved from 0.50707 to 0.50177, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3233 - accuracy: 0.8652 - precision: 0.8673 - recall: 0.8623 - val_loss: 0.5018 - val_accuracy: 0.7750 - val_precision: 0.7760 - val_recall: 0.7740\n",
            "Epoch 51/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.3117 - accuracy: 0.8686 - precision: 0.8704 - recall: 0.8662\n",
            "Epoch 51: val_loss did not improve from 0.50177\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.3108 - accuracy: 0.8687 - precision: 0.8705 - recall: 0.8664 - val_loss: 0.5071 - val_accuracy: 0.7696 - val_precision: 0.7698 - val_recall: 0.7686\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.8706 - precision: 0.8723 - recall: 0.8671\n",
            "Epoch 52: val_loss did not improve from 0.50177\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.3096 - accuracy: 0.8706 - precision: 0.8723 - recall: 0.8671 - val_loss: 0.5599 - val_accuracy: 0.7417 - val_precision: 0.7418 - val_recall: 0.7406\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8685 - precision: 0.8710 - recall: 0.8661\n",
            "Epoch 53: val_loss did not improve from 0.50177\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.3047 - accuracy: 0.8685 - precision: 0.8710 - recall: 0.8661 - val_loss: 0.5380 - val_accuracy: 0.7449 - val_precision: 0.7452 - val_recall: 0.7443\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8713 - precision: 0.8725 - recall: 0.8695\n",
            "Epoch 54: val_loss did not improve from 0.50177\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3046 - accuracy: 0.8713 - precision: 0.8725 - recall: 0.8695 - val_loss: 0.5701 - val_accuracy: 0.7400 - val_precision: 0.7408 - val_recall: 0.7400\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8707 - precision: 0.8737 - recall: 0.8691\n",
            "Epoch 55: val_loss did not improve from 0.50177\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3083 - accuracy: 0.8707 - precision: 0.8737 - recall: 0.8691 - val_loss: 0.5349 - val_accuracy: 0.7557 - val_precision: 0.7559 - val_recall: 0.7551\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8669 - precision: 0.8690 - recall: 0.8644\n",
            "Epoch 56: val_loss improved from 0.50177 to 0.46738, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3117 - accuracy: 0.8669 - precision: 0.8690 - recall: 0.8644 - val_loss: 0.4674 - val_accuracy: 0.7847 - val_precision: 0.7856 - val_recall: 0.7831\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8733 - precision: 0.8749 - recall: 0.8697\n",
            "Epoch 57: val_loss did not improve from 0.46738\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.3041 - accuracy: 0.8733 - precision: 0.8749 - recall: 0.8697 - val_loss: 0.5947 - val_accuracy: 0.7341 - val_precision: 0.7349 - val_recall: 0.7325\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8748 - precision: 0.8765 - recall: 0.8726\n",
            "Epoch 58: val_loss improved from 0.46738 to 0.46108, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 0.3007 - accuracy: 0.8748 - precision: 0.8765 - recall: 0.8726 - val_loss: 0.4611 - val_accuracy: 0.7966 - val_precision: 0.7972 - val_recall: 0.7955\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8809 - precision: 0.8823 - recall: 0.8780\n",
            "Epoch 59: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.2919 - accuracy: 0.8809 - precision: 0.8823 - recall: 0.8780 - val_loss: 0.4643 - val_accuracy: 0.7815 - val_precision: 0.7824 - val_recall: 0.7799\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8787 - precision: 0.8803 - recall: 0.8772\n",
            "Epoch 60: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2878 - accuracy: 0.8787 - precision: 0.8803 - recall: 0.8772 - val_loss: 0.4830 - val_accuracy: 0.7826 - val_precision: 0.7837 - val_recall: 0.7820\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.8809 - precision: 0.8822 - recall: 0.8780\n",
            "Epoch 61: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.2785 - accuracy: 0.8809 - precision: 0.8822 - recall: 0.8780 - val_loss: 0.5200 - val_accuracy: 0.7670 - val_precision: 0.7672 - val_recall: 0.7664\n",
            "Epoch 62/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2834 - accuracy: 0.8826 - precision: 0.8840 - recall: 0.8804\n",
            "Epoch 62: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.2844 - accuracy: 0.8823 - precision: 0.8837 - recall: 0.8799 - val_loss: 0.4994 - val_accuracy: 0.7750 - val_precision: 0.7757 - val_recall: 0.7745\n",
            "Epoch 63/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2855 - accuracy: 0.8787 - precision: 0.8798 - recall: 0.8766\n",
            "Epoch 63: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.2865 - accuracy: 0.8787 - precision: 0.8799 - recall: 0.8766 - val_loss: 0.4752 - val_accuracy: 0.7933 - val_precision: 0.7936 - val_recall: 0.7928\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.8847 - precision: 0.8860 - recall: 0.8826\n",
            "Epoch 64: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2802 - accuracy: 0.8847 - precision: 0.8860 - recall: 0.8826 - val_loss: 0.5195 - val_accuracy: 0.7686 - val_precision: 0.7702 - val_recall: 0.7686\n",
            "Epoch 65/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2742 - accuracy: 0.8852 - precision: 0.8867 - recall: 0.8823\n",
            "Epoch 65: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2748 - accuracy: 0.8845 - precision: 0.8860 - recall: 0.8816 - val_loss: 0.5057 - val_accuracy: 0.7777 - val_precision: 0.7769 - val_recall: 0.7740\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8851 - precision: 0.8867 - recall: 0.8835\n",
            "Epoch 66: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.2784 - accuracy: 0.8851 - precision: 0.8867 - recall: 0.8835 - val_loss: 0.5152 - val_accuracy: 0.7729 - val_precision: 0.7740 - val_recall: 0.7723\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8827 - precision: 0.8845 - recall: 0.8813\n",
            "Epoch 67: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.2784 - accuracy: 0.8827 - precision: 0.8845 - recall: 0.8813 - val_loss: 0.5501 - val_accuracy: 0.7610 - val_precision: 0.7619 - val_recall: 0.7594\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8841 - precision: 0.8853 - recall: 0.8824\n",
            "Epoch 68: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2759 - accuracy: 0.8841 - precision: 0.8853 - recall: 0.8824 - val_loss: 0.4751 - val_accuracy: 0.7809 - val_precision: 0.7814 - val_recall: 0.7809\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.8861 - precision: 0.8874 - recall: 0.8846\n",
            "Epoch 69: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.2758 - accuracy: 0.8861 - precision: 0.8874 - recall: 0.8846 - val_loss: 0.4982 - val_accuracy: 0.7815 - val_precision: 0.7832 - val_recall: 0.7815\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8888 - precision: 0.8896 - recall: 0.8866\n",
            "Epoch 70: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.2759 - accuracy: 0.8888 - precision: 0.8896 - recall: 0.8866 - val_loss: 0.5154 - val_accuracy: 0.7853 - val_precision: 0.7857 - val_recall: 0.7853\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.8915 - precision: 0.8924 - recall: 0.8907\n",
            "Epoch 71: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2658 - accuracy: 0.8915 - precision: 0.8924 - recall: 0.8907 - val_loss: 0.4911 - val_accuracy: 0.7858 - val_precision: 0.7860 - val_recall: 0.7847\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.8872 - precision: 0.8882 - recall: 0.8862\n",
            "Epoch 72: val_loss did not improve from 0.46108\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2633 - accuracy: 0.8872 - precision: 0.8882 - recall: 0.8862 - val_loss: 0.5064 - val_accuracy: 0.7890 - val_precision: 0.7898 - val_recall: 0.7885\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8920 - precision: 0.8932 - recall: 0.8903\n",
            "Epoch 73: val_loss improved from 0.46108 to 0.45606, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.2671 - accuracy: 0.8920 - precision: 0.8932 - recall: 0.8903 - val_loss: 0.4561 - val_accuracy: 0.8057 - val_precision: 0.8055 - val_recall: 0.8046\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8921 - precision: 0.8933 - recall: 0.8911\n",
            "Epoch 74: val_loss did not improve from 0.45606\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2554 - accuracy: 0.8921 - precision: 0.8933 - recall: 0.8911 - val_loss: 0.4857 - val_accuracy: 0.7982 - val_precision: 0.7984 - val_recall: 0.7971\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.8955 - precision: 0.8970 - recall: 0.8939\n",
            "Epoch 75: val_loss improved from 0.45606 to 0.44494, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.2525 - accuracy: 0.8955 - precision: 0.8970 - recall: 0.8939 - val_loss: 0.4449 - val_accuracy: 0.8132 - val_precision: 0.8141 - val_recall: 0.8132\n",
            "Epoch 76/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2465 - accuracy: 0.9003 - precision: 0.9010 - recall: 0.8984\n",
            "Epoch 76: val_loss did not improve from 0.44494\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2476 - accuracy: 0.8990 - precision: 0.8997 - recall: 0.8971 - val_loss: 0.4952 - val_accuracy: 0.7874 - val_precision: 0.7883 - val_recall: 0.7874\n",
            "Epoch 77/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2382 - accuracy: 0.8990 - precision: 0.9001 - recall: 0.8976\n",
            "Epoch 77: val_loss did not improve from 0.44494\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2399 - accuracy: 0.8982 - precision: 0.8992 - recall: 0.8967 - val_loss: 0.4625 - val_accuracy: 0.8138 - val_precision: 0.8135 - val_recall: 0.8122\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.8945 - precision: 0.8953 - recall: 0.8929\n",
            "Epoch 78: val_loss improved from 0.44494 to 0.43113, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.2516 - accuracy: 0.8945 - precision: 0.8953 - recall: 0.8929 - val_loss: 0.4311 - val_accuracy: 0.8143 - val_precision: 0.8149 - val_recall: 0.8127\n",
            "Epoch 79/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2468 - accuracy: 0.8956 - precision: 0.8963 - recall: 0.8945\n",
            "Epoch 79: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2464 - accuracy: 0.8956 - precision: 0.8962 - recall: 0.8945 - val_loss: 0.4776 - val_accuracy: 0.8127 - val_precision: 0.8130 - val_recall: 0.8122\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.8971 - precision: 0.8980 - recall: 0.8956\n",
            "Epoch 80: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2467 - accuracy: 0.8971 - precision: 0.8980 - recall: 0.8956 - val_loss: 0.4592 - val_accuracy: 0.8068 - val_precision: 0.8066 - val_recall: 0.8057\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.8977 - precision: 0.8987 - recall: 0.8966\n",
            "Epoch 81: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2495 - accuracy: 0.8977 - precision: 0.8987 - recall: 0.8966 - val_loss: 0.4521 - val_accuracy: 0.8143 - val_precision: 0.8142 - val_recall: 0.8138\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9002 - precision: 0.9015 - recall: 0.8990\n",
            "Epoch 82: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2427 - accuracy: 0.9002 - precision: 0.9015 - recall: 0.8990 - val_loss: 0.5027 - val_accuracy: 0.7879 - val_precision: 0.7879 - val_recall: 0.7858\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.8983 - precision: 0.8992 - recall: 0.8975\n",
            "Epoch 83: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2418 - accuracy: 0.8983 - precision: 0.8992 - recall: 0.8975 - val_loss: 0.4800 - val_accuracy: 0.7971 - val_precision: 0.7987 - val_recall: 0.7966\n",
            "Epoch 84/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2323 - accuracy: 0.9053 - precision: 0.9060 - recall: 0.9038\n",
            "Epoch 84: val_loss did not improve from 0.43113\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.2341 - accuracy: 0.9051 - precision: 0.9058 - recall: 0.9037 - val_loss: 0.5464 - val_accuracy: 0.7750 - val_precision: 0.7763 - val_recall: 0.7750\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9013 - precision: 0.9022 - recall: 0.9004\n",
            "Epoch 85: val_loss improved from 0.43113 to 0.41523, saving model to ./_best_weights.h5\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.2433 - accuracy: 0.9013 - precision: 0.9022 - recall: 0.9004 - val_loss: 0.4152 - val_accuracy: 0.8321 - val_precision: 0.8321 - val_recall: 0.8321\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9040 - precision: 0.9048 - recall: 0.9027\n",
            "Epoch 86: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2339 - accuracy: 0.9040 - precision: 0.9048 - recall: 0.9027 - val_loss: 0.4904 - val_accuracy: 0.8036 - val_precision: 0.8040 - val_recall: 0.8036\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9004 - precision: 0.9015 - recall: 0.8998\n",
            "Epoch 87: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2411 - accuracy: 0.9004 - precision: 0.9015 - recall: 0.8998 - val_loss: 0.4790 - val_accuracy: 0.7992 - val_precision: 0.8010 - val_recall: 0.7992\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.8986 - precision: 0.8995 - recall: 0.8973\n",
            "Epoch 88: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2472 - accuracy: 0.8986 - precision: 0.8995 - recall: 0.8973 - val_loss: 0.4626 - val_accuracy: 0.8057 - val_precision: 0.8065 - val_recall: 0.8052\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.9014 - precision: 0.9024 - recall: 0.9007\n",
            "Epoch 89: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2422 - accuracy: 0.9014 - precision: 0.9024 - recall: 0.9007 - val_loss: 0.4560 - val_accuracy: 0.8111 - val_precision: 0.8111 - val_recall: 0.8111\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9055 - precision: 0.9063 - recall: 0.9046\n",
            "Epoch 90: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2318 - accuracy: 0.9055 - precision: 0.9063 - recall: 0.9046 - val_loss: 0.4660 - val_accuracy: 0.8288 - val_precision: 0.8288 - val_recall: 0.8288\n",
            "Epoch 91/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2277 - accuracy: 0.9081 - precision: 0.9094 - recall: 0.9072\n",
            "Epoch 91: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2290 - accuracy: 0.9073 - precision: 0.9086 - recall: 0.9063 - val_loss: 0.4793 - val_accuracy: 0.8084 - val_precision: 0.8088 - val_recall: 0.8084\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9102 - precision: 0.9112 - recall: 0.9093\n",
            "Epoch 92: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2237 - accuracy: 0.9102 - precision: 0.9112 - recall: 0.9093 - val_loss: 0.4855 - val_accuracy: 0.7976 - val_precision: 0.7980 - val_recall: 0.7971\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9047 - precision: 0.9052 - recall: 0.9037\n",
            "Epoch 93: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2301 - accuracy: 0.9047 - precision: 0.9052 - recall: 0.9037 - val_loss: 0.4469 - val_accuracy: 0.8170 - val_precision: 0.8174 - val_recall: 0.8170\n",
            "Epoch 94/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2222 - accuracy: 0.9094 - precision: 0.9105 - recall: 0.9083\n",
            "Epoch 94: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2207 - accuracy: 0.9100 - precision: 0.9112 - recall: 0.9090 - val_loss: 0.5481 - val_accuracy: 0.7853 - val_precision: 0.7857 - val_recall: 0.7853\n",
            "Epoch 95/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2234 - accuracy: 0.9081 - precision: 0.9086 - recall: 0.9071\n",
            "Epoch 95: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.2257 - accuracy: 0.9072 - precision: 0.9077 - recall: 0.9062 - val_loss: 0.4419 - val_accuracy: 0.8143 - val_precision: 0.8154 - val_recall: 0.8132\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9080 - precision: 0.9088 - recall: 0.9070\n",
            "Epoch 96: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.2226 - accuracy: 0.9080 - precision: 0.9088 - recall: 0.9070 - val_loss: 0.4383 - val_accuracy: 0.8310 - val_precision: 0.8314 - val_recall: 0.8310\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.9091 - precision: 0.9101 - recall: 0.9084\n",
            "Epoch 97: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2223 - accuracy: 0.9091 - precision: 0.9101 - recall: 0.9084 - val_loss: 0.4752 - val_accuracy: 0.8116 - val_precision: 0.8115 - val_recall: 0.8111\n",
            "Epoch 98/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.9155 - precision: 0.9159 - recall: 0.9137\n",
            "Epoch 98: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2103 - accuracy: 0.9156 - precision: 0.9160 - recall: 0.9138 - val_loss: 0.5185 - val_accuracy: 0.7944 - val_precision: 0.7944 - val_recall: 0.7944\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9147 - precision: 0.9151 - recall: 0.9137\n",
            "Epoch 99: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.2060 - accuracy: 0.9147 - precision: 0.9151 - recall: 0.9137 - val_loss: 0.4827 - val_accuracy: 0.8143 - val_precision: 0.8152 - val_recall: 0.8143\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9189 - precision: 0.9196 - recall: 0.9173\n",
            "Epoch 100: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2024 - accuracy: 0.9189 - precision: 0.9196 - recall: 0.9173 - val_loss: 0.5153 - val_accuracy: 0.7939 - val_precision: 0.7950 - val_recall: 0.7933\n",
            "Epoch 101/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2045 - accuracy: 0.9135 - precision: 0.9143 - recall: 0.9124\n",
            "Epoch 101: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.2040 - accuracy: 0.9137 - precision: 0.9144 - recall: 0.9126 - val_loss: 0.4801 - val_accuracy: 0.7998 - val_precision: 0.8001 - val_recall: 0.7992\n",
            "Epoch 102/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2077 - accuracy: 0.9157 - precision: 0.9165 - recall: 0.9147\n",
            "Epoch 102: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2080 - accuracy: 0.9155 - precision: 0.9162 - recall: 0.9145 - val_loss: 0.5191 - val_accuracy: 0.7869 - val_precision: 0.7868 - val_recall: 0.7863\n",
            "Epoch 103/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2034 - accuracy: 0.9137 - precision: 0.9139 - recall: 0.9127\n",
            "Epoch 103: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.2030 - accuracy: 0.9142 - precision: 0.9146 - recall: 0.9133 - val_loss: 0.4457 - val_accuracy: 0.8213 - val_precision: 0.8213 - val_recall: 0.8213\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9198 - precision: 0.9203 - recall: 0.9192\n",
            "Epoch 104: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1995 - accuracy: 0.9198 - precision: 0.9203 - recall: 0.9192 - val_loss: 0.5598 - val_accuracy: 0.7750 - val_precision: 0.7747 - val_recall: 0.7734\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.9166 - precision: 0.9172 - recall: 0.9159\n",
            "Epoch 105: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.2068 - accuracy: 0.9166 - precision: 0.9172 - recall: 0.9159 - val_loss: 0.5095 - val_accuracy: 0.7933 - val_precision: 0.7933 - val_recall: 0.7933\n",
            "Epoch 106/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1950 - accuracy: 0.9202 - precision: 0.9208 - recall: 0.9195\n",
            "Epoch 106: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1962 - accuracy: 0.9200 - precision: 0.9206 - recall: 0.9193 - val_loss: 0.5272 - val_accuracy: 0.7960 - val_precision: 0.7969 - val_recall: 0.7960\n",
            "Epoch 107/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.2032 - accuracy: 0.9191 - precision: 0.9199 - recall: 0.9182\n",
            "Epoch 107: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.2040 - accuracy: 0.9190 - precision: 0.9197 - recall: 0.9180 - val_loss: 0.4877 - val_accuracy: 0.8154 - val_precision: 0.8158 - val_recall: 0.8154\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9178 - precision: 0.9183 - recall: 0.9173\n",
            "Epoch 108: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.2040 - accuracy: 0.9178 - precision: 0.9183 - recall: 0.9173 - val_loss: 0.5082 - val_accuracy: 0.8159 - val_precision: 0.8168 - val_recall: 0.8159\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9197 - precision: 0.9200 - recall: 0.9192\n",
            "Epoch 109: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1996 - accuracy: 0.9197 - precision: 0.9200 - recall: 0.9192 - val_loss: 0.4711 - val_accuracy: 0.8175 - val_precision: 0.8178 - val_recall: 0.8165\n",
            "Epoch 110/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1905 - accuracy: 0.9223 - precision: 0.9228 - recall: 0.9216\n",
            "Epoch 110: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1913 - accuracy: 0.9220 - precision: 0.9225 - recall: 0.9213 - val_loss: 0.5631 - val_accuracy: 0.7949 - val_precision: 0.7956 - val_recall: 0.7939\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9243 - precision: 0.9247 - recall: 0.9232\n",
            "Epoch 111: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1872 - accuracy: 0.9243 - precision: 0.9247 - recall: 0.9232 - val_loss: 0.5053 - val_accuracy: 0.8019 - val_precision: 0.8018 - val_recall: 0.8014\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9252 - precision: 0.9254 - recall: 0.9248\n",
            "Epoch 112: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1866 - accuracy: 0.9252 - precision: 0.9254 - recall: 0.9248 - val_loss: 0.5036 - val_accuracy: 0.8089 - val_precision: 0.8094 - val_recall: 0.8089\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9211 - precision: 0.9220 - recall: 0.9207\n",
            "Epoch 113: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1891 - accuracy: 0.9211 - precision: 0.9220 - recall: 0.9207 - val_loss: 0.5082 - val_accuracy: 0.7960 - val_precision: 0.7959 - val_recall: 0.7955\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9204 - precision: 0.9209 - recall: 0.9199\n",
            "Epoch 114: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1932 - accuracy: 0.9204 - precision: 0.9209 - recall: 0.9199 - val_loss: 0.5129 - val_accuracy: 0.8154 - val_precision: 0.8152 - val_recall: 0.8143\n",
            "Epoch 115/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1832 - accuracy: 0.9244 - precision: 0.9248 - recall: 0.9241\n",
            "Epoch 115: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1829 - accuracy: 0.9246 - precision: 0.9250 - recall: 0.9242 - val_loss: 0.5331 - val_accuracy: 0.8046 - val_precision: 0.8046 - val_recall: 0.8046\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9225 - precision: 0.9231 - recall: 0.9220\n",
            "Epoch 116: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.1905 - accuracy: 0.9225 - precision: 0.9231 - recall: 0.9220 - val_loss: 0.5568 - val_accuracy: 0.7976 - val_precision: 0.7978 - val_recall: 0.7966\n",
            "Epoch 117/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1869 - accuracy: 0.9219 - precision: 0.9223 - recall: 0.9212\n",
            "Epoch 117: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1867 - accuracy: 0.9219 - precision: 0.9223 - recall: 0.9213 - val_loss: 0.5489 - val_accuracy: 0.8089 - val_precision: 0.8097 - val_recall: 0.8084\n",
            "Epoch 118/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1834 - accuracy: 0.9289 - precision: 0.9292 - recall: 0.9284\n",
            "Epoch 118: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1832 - accuracy: 0.9289 - precision: 0.9293 - recall: 0.9285 - val_loss: 0.5462 - val_accuracy: 0.8057 - val_precision: 0.8070 - val_recall: 0.8057\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9263 - precision: 0.9267 - recall: 0.9251\n",
            "Epoch 119: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1779 - accuracy: 0.9263 - precision: 0.9267 - recall: 0.9251 - val_loss: 0.5773 - val_accuracy: 0.7901 - val_precision: 0.7907 - val_recall: 0.7890\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9306 - precision: 0.9312 - recall: 0.9304\n",
            "Epoch 120: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.1722 - accuracy: 0.9306 - precision: 0.9312 - recall: 0.9304 - val_loss: 0.5900 - val_accuracy: 0.7863 - val_precision: 0.7863 - val_recall: 0.7863\n",
            "Epoch 121/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1716 - accuracy: 0.9312 - precision: 0.9313 - recall: 0.9308\n",
            "Epoch 121: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 0s 46ms/step - loss: 0.1755 - accuracy: 0.9293 - precision: 0.9295 - recall: 0.9289 - val_loss: 0.5433 - val_accuracy: 0.8073 - val_precision: 0.8078 - val_recall: 0.8073\n",
            "Epoch 122/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1756 - accuracy: 0.9288 - precision: 0.9290 - recall: 0.9279\n",
            "Epoch 122: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1763 - accuracy: 0.9287 - precision: 0.9289 - recall: 0.9278 - val_loss: 0.5907 - val_accuracy: 0.7853 - val_precision: 0.7857 - val_recall: 0.7853\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9300 - precision: 0.9304 - recall: 0.9294\n",
            "Epoch 123: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.1750 - accuracy: 0.9300 - precision: 0.9304 - recall: 0.9294 - val_loss: 0.5351 - val_accuracy: 0.8052 - val_precision: 0.8068 - val_recall: 0.8046\n",
            "Epoch 124/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1770 - accuracy: 0.9299 - precision: 0.9307 - recall: 0.9295\n",
            "Epoch 124: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1774 - accuracy: 0.9294 - precision: 0.9302 - recall: 0.9290 - val_loss: 0.5507 - val_accuracy: 0.8009 - val_precision: 0.8006 - val_recall: 0.7998\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9293 - precision: 0.9297 - recall: 0.9290\n",
            "Epoch 125: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1789 - accuracy: 0.9293 - precision: 0.9297 - recall: 0.9290 - val_loss: 0.5551 - val_accuracy: 0.8003 - val_precision: 0.8002 - val_recall: 0.7998\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9244 - precision: 0.9250 - recall: 0.9239\n",
            "Epoch 126: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.1855 - accuracy: 0.9244 - precision: 0.9250 - recall: 0.9239 - val_loss: 0.5313 - val_accuracy: 0.8138 - val_precision: 0.8142 - val_recall: 0.8138\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9262 - precision: 0.9265 - recall: 0.9258\n",
            "Epoch 127: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1768 - accuracy: 0.9262 - precision: 0.9265 - recall: 0.9258 - val_loss: 0.4866 - val_accuracy: 0.8138 - val_precision: 0.8146 - val_recall: 0.8132\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9271 - precision: 0.9275 - recall: 0.9266\n",
            "Epoch 128: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1797 - accuracy: 0.9271 - precision: 0.9275 - recall: 0.9266 - val_loss: 0.5719 - val_accuracy: 0.8046 - val_precision: 0.8050 - val_recall: 0.8041\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9301 - precision: 0.9303 - recall: 0.9299\n",
            "Epoch 129: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1773 - accuracy: 0.9301 - precision: 0.9303 - recall: 0.9299 - val_loss: 0.4809 - val_accuracy: 0.8224 - val_precision: 0.8228 - val_recall: 0.8224\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9269 - precision: 0.9274 - recall: 0.9265\n",
            "Epoch 130: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1753 - accuracy: 0.9269 - precision: 0.9274 - recall: 0.9265 - val_loss: 0.5262 - val_accuracy: 0.8132 - val_precision: 0.8137 - val_recall: 0.8132\n",
            "Epoch 131/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1704 - accuracy: 0.9284 - precision: 0.9288 - recall: 0.9279\n",
            "Epoch 131: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1716 - accuracy: 0.9280 - precision: 0.9284 - recall: 0.9275 - val_loss: 0.5158 - val_accuracy: 0.8111 - val_precision: 0.8111 - val_recall: 0.8111\n",
            "Epoch 132/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1643 - accuracy: 0.9334 - precision: 0.9339 - recall: 0.9328\n",
            "Epoch 132: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1642 - accuracy: 0.9331 - precision: 0.9336 - recall: 0.9326 - val_loss: 0.5475 - val_accuracy: 0.8073 - val_precision: 0.8082 - val_recall: 0.8073\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9332 - precision: 0.9341 - recall: 0.9328\n",
            "Epoch 133: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1679 - accuracy: 0.9332 - precision: 0.9341 - recall: 0.9328 - val_loss: 0.4876 - val_accuracy: 0.8310 - val_precision: 0.8308 - val_recall: 0.8299\n",
            "Epoch 134/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1680 - accuracy: 0.9312 - precision: 0.9318 - recall: 0.9307\n",
            "Epoch 134: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1671 - accuracy: 0.9319 - precision: 0.9324 - recall: 0.9313 - val_loss: 0.5498 - val_accuracy: 0.8100 - val_precision: 0.8098 - val_recall: 0.8089\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9350 - precision: 0.9357 - recall: 0.9343\n",
            "Epoch 135: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1654 - accuracy: 0.9350 - precision: 0.9357 - recall: 0.9343 - val_loss: 0.5664 - val_accuracy: 0.7966 - val_precision: 0.7966 - val_recall: 0.7966\n",
            "Epoch 136/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1578 - accuracy: 0.9352 - precision: 0.9352 - recall: 0.9345\n",
            "Epoch 136: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1575 - accuracy: 0.9355 - precision: 0.9355 - recall: 0.9348 - val_loss: 0.6170 - val_accuracy: 0.7922 - val_precision: 0.7931 - val_recall: 0.7922\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.9376 - precision: 0.9377 - recall: 0.9370\n",
            "Epoch 137: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1588 - accuracy: 0.9376 - precision: 0.9377 - recall: 0.9370 - val_loss: 0.5532 - val_accuracy: 0.7960 - val_precision: 0.7964 - val_recall: 0.7960\n",
            "Epoch 138/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9385 - precision: 0.9390 - recall: 0.9380\n",
            "Epoch 138: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1568 - accuracy: 0.9381 - precision: 0.9386 - recall: 0.9376 - val_loss: 0.5447 - val_accuracy: 0.8062 - val_precision: 0.8075 - val_recall: 0.8062\n",
            "Epoch 139/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.9341 - precision: 0.9348 - recall: 0.9337\n",
            "Epoch 139: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1630 - accuracy: 0.9343 - precision: 0.9350 - recall: 0.9339 - val_loss: 0.5994 - val_accuracy: 0.7982 - val_precision: 0.7981 - val_recall: 0.7976\n",
            "Epoch 140/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1574 - accuracy: 0.9378 - precision: 0.9381 - recall: 0.9373\n",
            "Epoch 140: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1581 - accuracy: 0.9380 - precision: 0.9383 - recall: 0.9374 - val_loss: 0.5667 - val_accuracy: 0.8089 - val_precision: 0.8094 - val_recall: 0.8089\n",
            "Epoch 141/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1578 - accuracy: 0.9368 - precision: 0.9373 - recall: 0.9362\n",
            "Epoch 141: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1573 - accuracy: 0.9370 - precision: 0.9375 - recall: 0.9364 - val_loss: 0.5216 - val_accuracy: 0.8052 - val_precision: 0.8054 - val_recall: 0.8041\n",
            "Epoch 142/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1573 - accuracy: 0.9368 - precision: 0.9376 - recall: 0.9362\n",
            "Epoch 142: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1567 - accuracy: 0.9372 - precision: 0.9380 - recall: 0.9366 - val_loss: 0.5530 - val_accuracy: 0.8095 - val_precision: 0.8095 - val_recall: 0.8095\n",
            "Epoch 143/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1578 - accuracy: 0.9372 - precision: 0.9377 - recall: 0.9368\n",
            "Epoch 143: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1576 - accuracy: 0.9371 - precision: 0.9376 - recall: 0.9367 - val_loss: 0.5675 - val_accuracy: 0.8062 - val_precision: 0.8062 - val_recall: 0.8062\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.9417 - precision: 0.9421 - recall: 0.9413\n",
            "Epoch 144: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1494 - accuracy: 0.9417 - precision: 0.9421 - recall: 0.9413 - val_loss: 0.5586 - val_accuracy: 0.8079 - val_precision: 0.8079 - val_recall: 0.8079\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9384 - precision: 0.9391 - recall: 0.9383\n",
            "Epoch 145: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1501 - accuracy: 0.9384 - precision: 0.9391 - recall: 0.9383 - val_loss: 0.5817 - val_accuracy: 0.8003 - val_precision: 0.8008 - val_recall: 0.8003\n",
            "Epoch 146/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1648 - accuracy: 0.9341 - precision: 0.9346 - recall: 0.9333\n",
            "Epoch 146: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1651 - accuracy: 0.9338 - precision: 0.9344 - recall: 0.9330 - val_loss: 0.5881 - val_accuracy: 0.8052 - val_precision: 0.8056 - val_recall: 0.8052\n",
            "Epoch 147/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1613 - accuracy: 0.9343 - precision: 0.9346 - recall: 0.9339\n",
            "Epoch 147: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1611 - accuracy: 0.9345 - precision: 0.9348 - recall: 0.9341 - val_loss: 0.5462 - val_accuracy: 0.8116 - val_precision: 0.8114 - val_recall: 0.8105\n",
            "Epoch 148/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1542 - accuracy: 0.9389 - precision: 0.9395 - recall: 0.9384\n",
            "Epoch 148: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1575 - accuracy: 0.9375 - precision: 0.9381 - recall: 0.9370 - val_loss: 0.5866 - val_accuracy: 0.7982 - val_precision: 0.7986 - val_recall: 0.7982\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9361 - precision: 0.9365 - recall: 0.9357\n",
            "Epoch 149: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1606 - accuracy: 0.9361 - precision: 0.9365 - recall: 0.9357 - val_loss: 0.5046 - val_accuracy: 0.8229 - val_precision: 0.8233 - val_recall: 0.8224\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9347 - precision: 0.9353 - recall: 0.9345\n",
            "Epoch 150: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1629 - accuracy: 0.9347 - precision: 0.9353 - recall: 0.9345 - val_loss: 0.5674 - val_accuracy: 0.8138 - val_precision: 0.8138 - val_recall: 0.8138\n",
            "Epoch 151/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1495 - accuracy: 0.9394 - precision: 0.9398 - recall: 0.9388\n",
            "Epoch 151: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.1508 - accuracy: 0.9387 - precision: 0.9392 - recall: 0.9382 - val_loss: 0.6482 - val_accuracy: 0.7949 - val_precision: 0.7954 - val_recall: 0.7949\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9426 - precision: 0.9431 - recall: 0.9422\n",
            "Epoch 152: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1460 - accuracy: 0.9426 - precision: 0.9431 - recall: 0.9422 - val_loss: 0.5808 - val_accuracy: 0.8132 - val_precision: 0.8131 - val_recall: 0.8127\n",
            "Epoch 153/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1414 - accuracy: 0.9452 - precision: 0.9452 - recall: 0.9452\n",
            "Epoch 153: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1400 - accuracy: 0.9458 - precision: 0.9458 - recall: 0.9458 - val_loss: 0.6241 - val_accuracy: 0.7998 - val_precision: 0.8015 - val_recall: 0.7998\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9443 - precision: 0.9449 - recall: 0.9440\n",
            "Epoch 154: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.1399 - accuracy: 0.9443 - precision: 0.9449 - recall: 0.9440 - val_loss: 0.5797 - val_accuracy: 0.8159 - val_precision: 0.8159 - val_recall: 0.8159\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9462 - precision: 0.9466 - recall: 0.9458\n",
            "Epoch 155: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.1388 - accuracy: 0.9462 - precision: 0.9466 - recall: 0.9458 - val_loss: 0.5602 - val_accuracy: 0.8105 - val_precision: 0.8103 - val_recall: 0.8095\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9398 - precision: 0.9400 - recall: 0.9396\n",
            "Epoch 156: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 0s 45ms/step - loss: 0.1515 - accuracy: 0.9398 - precision: 0.9400 - recall: 0.9396 - val_loss: 0.5676 - val_accuracy: 0.8143 - val_precision: 0.8148 - val_recall: 0.8143\n",
            "Epoch 157/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9440 - precision: 0.9444 - recall: 0.9438\n",
            "Epoch 157: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1417 - accuracy: 0.9440 - precision: 0.9444 - recall: 0.9438 - val_loss: 0.5945 - val_accuracy: 0.8068 - val_precision: 0.8072 - val_recall: 0.8068\n",
            "Epoch 158/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1414 - accuracy: 0.9439 - precision: 0.9444 - recall: 0.9438\n",
            "Epoch 158: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1416 - accuracy: 0.9434 - precision: 0.9438 - recall: 0.9433 - val_loss: 0.5939 - val_accuracy: 0.7960 - val_precision: 0.7964 - val_recall: 0.7960\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9411 - precision: 0.9413 - recall: 0.9407\n",
            "Epoch 159: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1481 - accuracy: 0.9411 - precision: 0.9413 - recall: 0.9407 - val_loss: 0.5388 - val_accuracy: 0.8154 - val_precision: 0.8163 - val_recall: 0.8154\n",
            "Epoch 160/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1550 - accuracy: 0.9410 - precision: 0.9415 - recall: 0.9406\n",
            "Epoch 160: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1560 - accuracy: 0.9406 - precision: 0.9410 - recall: 0.9402 - val_loss: 0.5552 - val_accuracy: 0.8036 - val_precision: 0.8044 - val_recall: 0.8036\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9432 - precision: 0.9433 - recall: 0.9431\n",
            "Epoch 161: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1421 - accuracy: 0.9432 - precision: 0.9433 - recall: 0.9431 - val_loss: 0.5452 - val_accuracy: 0.8132 - val_precision: 0.8131 - val_recall: 0.8127\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.9447 - precision: 0.9450 - recall: 0.9444\n",
            "Epoch 162: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1429 - accuracy: 0.9447 - precision: 0.9450 - recall: 0.9444 - val_loss: 0.5490 - val_accuracy: 0.8181 - val_precision: 0.8180 - val_recall: 0.8175\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9440 - precision: 0.9441 - recall: 0.9437\n",
            "Epoch 163: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1375 - accuracy: 0.9440 - precision: 0.9441 - recall: 0.9437 - val_loss: 0.5627 - val_accuracy: 0.8175 - val_precision: 0.8175 - val_recall: 0.8175\n",
            "Epoch 164/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1360 - accuracy: 0.9468 - precision: 0.9472 - recall: 0.9463\n",
            "Epoch 164: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1367 - accuracy: 0.9470 - precision: 0.9474 - recall: 0.9465 - val_loss: 0.6166 - val_accuracy: 0.8009 - val_precision: 0.8009 - val_recall: 0.8009\n",
            "Epoch 165/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1479 - accuracy: 0.9409 - precision: 0.9412 - recall: 0.9406\n",
            "Epoch 165: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1487 - accuracy: 0.9402 - precision: 0.9404 - recall: 0.9399 - val_loss: 0.5308 - val_accuracy: 0.8267 - val_precision: 0.8271 - val_recall: 0.8267\n",
            "Epoch 166/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1375 - accuracy: 0.9459 - precision: 0.9462 - recall: 0.9457\n",
            "Epoch 166: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1393 - accuracy: 0.9454 - precision: 0.9456 - recall: 0.9451 - val_loss: 0.6063 - val_accuracy: 0.8111 - val_precision: 0.8114 - val_recall: 0.8105\n",
            "Epoch 167/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1323 - accuracy: 0.9478 - precision: 0.9480 - recall: 0.9478\n",
            "Epoch 167: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1326 - accuracy: 0.9478 - precision: 0.9480 - recall: 0.9478 - val_loss: 0.5559 - val_accuracy: 0.8116 - val_precision: 0.8119 - val_recall: 0.8105\n",
            "Epoch 168/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1307 - accuracy: 0.9488 - precision: 0.9490 - recall: 0.9485\n",
            "Epoch 168: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1311 - accuracy: 0.9491 - precision: 0.9492 - recall: 0.9488 - val_loss: 0.6155 - val_accuracy: 0.7982 - val_precision: 0.7986 - val_recall: 0.7982\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9499 - precision: 0.9502 - recall: 0.9497\n",
            "Epoch 169: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.1275 - accuracy: 0.9499 - precision: 0.9502 - recall: 0.9497 - val_loss: 0.6182 - val_accuracy: 0.8132 - val_precision: 0.8132 - val_recall: 0.8132\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9467 - precision: 0.9472 - recall: 0.9465\n",
            "Epoch 170: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1380 - accuracy: 0.9467 - precision: 0.9472 - recall: 0.9465 - val_loss: 0.6090 - val_accuracy: 0.8052 - val_precision: 0.8056 - val_recall: 0.8052\n",
            "Epoch 171/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1396 - accuracy: 0.9479 - precision: 0.9481 - recall: 0.9476\n",
            "Epoch 171: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1386 - accuracy: 0.9482 - precision: 0.9484 - recall: 0.9478 - val_loss: 0.5429 - val_accuracy: 0.8224 - val_precision: 0.8233 - val_recall: 0.8224\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9449 - precision: 0.9452 - recall: 0.9441\n",
            "Epoch 172: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 46ms/step - loss: 0.1385 - accuracy: 0.9449 - precision: 0.9452 - recall: 0.9441 - val_loss: 0.5721 - val_accuracy: 0.8111 - val_precision: 0.8120 - val_recall: 0.8111\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9503 - precision: 0.9507 - recall: 0.9497\n",
            "Epoch 173: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1292 - accuracy: 0.9503 - precision: 0.9507 - recall: 0.9497 - val_loss: 0.5970 - val_accuracy: 0.8165 - val_precision: 0.8169 - val_recall: 0.8165\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9470 - precision: 0.9470 - recall: 0.9464\n",
            "Epoch 174: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1341 - accuracy: 0.9470 - precision: 0.9470 - recall: 0.9464 - val_loss: 0.6635 - val_accuracy: 0.7922 - val_precision: 0.7922 - val_recall: 0.7922\n",
            "Epoch 175/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1410 - accuracy: 0.9421 - precision: 0.9425 - recall: 0.9417\n",
            "Epoch 175: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1410 - accuracy: 0.9421 - precision: 0.9427 - recall: 0.9418 - val_loss: 0.6110 - val_accuracy: 0.8003 - val_precision: 0.8003 - val_recall: 0.8003\n",
            "Epoch 176/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1264 - accuracy: 0.9477 - precision: 0.9479 - recall: 0.9474\n",
            "Epoch 176: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1273 - accuracy: 0.9473 - precision: 0.9475 - recall: 0.9470 - val_loss: 0.6577 - val_accuracy: 0.7928 - val_precision: 0.7936 - val_recall: 0.7928\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9492 - precision: 0.9493 - recall: 0.9490\n",
            "Epoch 177: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1232 - accuracy: 0.9492 - precision: 0.9493 - recall: 0.9490 - val_loss: 0.6433 - val_accuracy: 0.7976 - val_precision: 0.7985 - val_recall: 0.7976\n",
            "Epoch 178/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1119 - accuracy: 0.9546 - precision: 0.9548 - recall: 0.9545\n",
            "Epoch 178: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1119 - accuracy: 0.9548 - precision: 0.9550 - recall: 0.9547 - val_loss: 0.7602 - val_accuracy: 0.7799 - val_precision: 0.7798 - val_recall: 0.7793\n",
            "Epoch 179/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1240 - accuracy: 0.9528 - precision: 0.9530 - recall: 0.9526\n",
            "Epoch 179: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1256 - accuracy: 0.9521 - precision: 0.9523 - recall: 0.9519 - val_loss: 0.5930 - val_accuracy: 0.8122 - val_precision: 0.8121 - val_recall: 0.8116\n",
            "Epoch 180/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1293 - accuracy: 0.9507 - precision: 0.9508 - recall: 0.9501\n",
            "Epoch 180: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1311 - accuracy: 0.9498 - precision: 0.9500 - recall: 0.9493 - val_loss: 0.6039 - val_accuracy: 0.8143 - val_precision: 0.8146 - val_recall: 0.8132\n",
            "Epoch 181/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1322 - accuracy: 0.9477 - precision: 0.9480 - recall: 0.9475\n",
            "Epoch 181: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1324 - accuracy: 0.9474 - precision: 0.9477 - recall: 0.9472 - val_loss: 0.6401 - val_accuracy: 0.8062 - val_precision: 0.8067 - val_recall: 0.8062\n",
            "Epoch 182/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9516 - precision: 0.9518 - recall: 0.9512\n",
            "Epoch 182: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1288 - accuracy: 0.9510 - precision: 0.9512 - recall: 0.9506 - val_loss: 0.6261 - val_accuracy: 0.8079 - val_precision: 0.8081 - val_recall: 0.8068\n",
            "Epoch 183/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1164 - accuracy: 0.9569 - precision: 0.9570 - recall: 0.9568\n",
            "Epoch 183: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1173 - accuracy: 0.9563 - precision: 0.9564 - recall: 0.9562 - val_loss: 0.6501 - val_accuracy: 0.8084 - val_precision: 0.8083 - val_recall: 0.8079\n",
            "Epoch 184/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1210 - accuracy: 0.9537 - precision: 0.9541 - recall: 0.9535\n",
            "Epoch 184: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1228 - accuracy: 0.9529 - precision: 0.9533 - recall: 0.9527 - val_loss: 0.7049 - val_accuracy: 0.7928 - val_precision: 0.7931 - val_recall: 0.7922\n",
            "Epoch 185/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1319 - accuracy: 0.9479 - precision: 0.9483 - recall: 0.9474\n",
            "Epoch 185: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1327 - accuracy: 0.9473 - precision: 0.9477 - recall: 0.9468 - val_loss: 0.5956 - val_accuracy: 0.8149 - val_precision: 0.8153 - val_recall: 0.8149\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9514 - precision: 0.9520 - recall: 0.9511\n",
            "Epoch 186: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1280 - accuracy: 0.9514 - precision: 0.9520 - recall: 0.9511 - val_loss: 0.6421 - val_accuracy: 0.7998 - val_precision: 0.7997 - val_recall: 0.7992\n",
            "Epoch 187/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1205 - accuracy: 0.9542 - precision: 0.9543 - recall: 0.9540\n",
            "Epoch 187: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 47ms/step - loss: 0.1225 - accuracy: 0.9532 - precision: 0.9532 - recall: 0.9530 - val_loss: 0.5879 - val_accuracy: 0.8159 - val_precision: 0.8159 - val_recall: 0.8159\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9517 - precision: 0.9519 - recall: 0.9516\n",
            "Epoch 188: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1252 - accuracy: 0.9517 - precision: 0.9519 - recall: 0.9516 - val_loss: 0.6358 - val_accuracy: 0.8052 - val_precision: 0.8059 - val_recall: 0.8046\n",
            "Epoch 189/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1156 - accuracy: 0.9567 - precision: 0.9570 - recall: 0.9565\n",
            "Epoch 189: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1167 - accuracy: 0.9559 - precision: 0.9562 - recall: 0.9557 - val_loss: 0.6725 - val_accuracy: 0.7906 - val_precision: 0.7906 - val_recall: 0.7906\n",
            "Epoch 190/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1261 - accuracy: 0.9515 - precision: 0.9515 - recall: 0.9514\n",
            "Epoch 190: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1259 - accuracy: 0.9520 - precision: 0.9520 - recall: 0.9519 - val_loss: 0.6572 - val_accuracy: 0.8062 - val_precision: 0.8061 - val_recall: 0.8057\n",
            "Epoch 191/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1092 - accuracy: 0.9570 - precision: 0.9574 - recall: 0.9567\n",
            "Epoch 191: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1103 - accuracy: 0.9568 - precision: 0.9571 - recall: 0.9565 - val_loss: 0.6749 - val_accuracy: 0.7928 - val_precision: 0.7928 - val_recall: 0.7928\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9562 - precision: 0.9562 - recall: 0.9557\n",
            "Epoch 192: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1154 - accuracy: 0.9562 - precision: 0.9562 - recall: 0.9557 - val_loss: 0.6517 - val_accuracy: 0.8062 - val_precision: 0.8062 - val_recall: 0.8062\n",
            "Epoch 193/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1216 - accuracy: 0.9537 - precision: 0.9538 - recall: 0.9536\n",
            "Epoch 193: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1246 - accuracy: 0.9528 - precision: 0.9530 - recall: 0.9526 - val_loss: 0.6129 - val_accuracy: 0.8165 - val_precision: 0.8165 - val_recall: 0.8165\n",
            "Epoch 194/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1185 - accuracy: 0.9543 - precision: 0.9547 - recall: 0.9540\n",
            "Epoch 194: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1200 - accuracy: 0.9535 - precision: 0.9540 - recall: 0.9533 - val_loss: 0.5996 - val_accuracy: 0.8116 - val_precision: 0.8116 - val_recall: 0.8116\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9515 - precision: 0.9520 - recall: 0.9512\n",
            "Epoch 195: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.1239 - accuracy: 0.9515 - precision: 0.9520 - recall: 0.9512 - val_loss: 0.5748 - val_accuracy: 0.8170 - val_precision: 0.8169 - val_recall: 0.8165\n",
            "Epoch 196/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1163 - accuracy: 0.9557 - precision: 0.9562 - recall: 0.9553\n",
            "Epoch 196: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.1169 - accuracy: 0.9554 - precision: 0.9561 - recall: 0.9551 - val_loss: 0.6458 - val_accuracy: 0.8014 - val_precision: 0.8014 - val_recall: 0.8014\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9523 - precision: 0.9526 - recall: 0.9516\n",
            "Epoch 197: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1209 - accuracy: 0.9523 - precision: 0.9526 - recall: 0.9516 - val_loss: 0.5994 - val_accuracy: 0.8251 - val_precision: 0.8251 - val_recall: 0.8251\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9510 - precision: 0.9514 - recall: 0.9506\n",
            "Epoch 198: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1225 - accuracy: 0.9510 - precision: 0.9514 - recall: 0.9506 - val_loss: 0.5993 - val_accuracy: 0.8122 - val_precision: 0.8130 - val_recall: 0.8122\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9547 - precision: 0.9549 - recall: 0.9544\n",
            "Epoch 199: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1164 - accuracy: 0.9547 - precision: 0.9549 - recall: 0.9544 - val_loss: 0.6203 - val_accuracy: 0.8079 - val_precision: 0.8087 - val_recall: 0.8079\n",
            "Epoch 200/200\n",
            "10/11 [==========================>...] - ETA: 0s - loss: 0.1201 - accuracy: 0.9532 - precision: 0.9535 - recall: 0.9530\n",
            "Epoch 200: val_loss did not improve from 0.41523\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1211 - accuracy: 0.9529 - precision: 0.9531 - recall: 0.9527 - val_loss: 0.5964 - val_accuracy: 0.8143 - val_precision: 0.8148 - val_recall: 0.8143\n"
          ]
        }
      ],
      "source": [
        "checkpointer = ModelCheckpoint(filepath='./'+'_best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
        "\n",
        "history =model.fit(x_train_new, Y_train_new, epochs=200, batch_size=1024, verbose=1, validation_split=0.15, callbacks=[checkpointer])\n",
        "\n",
        "model.save('weights_cnn2_t1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSjYyMOIthHx"
      },
      "source": [
        "**Evalutations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0b8bFSIZGES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa88f486-1e30-41f2-992f-50de4e2d3b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 1s 10ms/step - loss: 0.5492 - accuracy: 0.8279 - precision: 0.8279 - recall: 0.8279\n",
            "[0.5491651296615601, 0.827886700630188, 0.827886700630188, 0.827886700630188]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "#print('Test loss:', score[0])\n",
        "#print('Test accuracy:', score[1])\n",
        "print (score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHmv-wR1ZJQ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "07bab513-4ee1-44c1-e536-1bdf61ff3132"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt # shorcut for a plot function\n",
        "plt.plot(history.history['accuracy']) # training accuracy\n",
        "plt.plot(history.history['val_accuracy']) # validation accuracy\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsuHy4BmZO43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "505aab32-e74e-46b2-9f9b-53c1c862f491"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hh3lutSZUWO"
      },
      "outputs": [],
      "source": [
        "\n",
        "predictions = model.predict(x_test_new,  verbose=1)\n",
        "print (\"predicted images size :\",predictions.shape)\n",
        "print(predictions)\n",
        "print(Y_test_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOnUn784ZZ_g"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score\n",
        "threshold_confusion = 0.5\n",
        "\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "y_pred = np.empty((predictions.shape[0]))\n",
        "y_test = np.empty((predictions.shape[0]))\n",
        "print(Y_test_new)\n",
        "for i in range(predictions.shape[0]):\n",
        "    #print(predictions[i])\n",
        "    y_pred[i]=np.argmax(predictions[i])\n",
        "    y_test[i]=np.argmax(Y_test_new[i])\n",
        "print(y_pred)\n",
        "print(y_test)  \n",
        "       \n",
        "confusion = confusion_matrix(y_test,  y_pred)\n",
        "\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_B6tLCZiuxm"
      },
      "outputs": [],
      "source": [
        "z=[]\n",
        "for i in range(len(y_pred_all)):\n",
        "    if(y_pred_all[i][0]==1):\n",
        "        z.append(i)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM74mlnSiQnB"
      },
      "outputs": [],
      "source": [
        "d=[63,2025]\n",
        "chb=[1864, 5763]\n",
        "d=numpy.array(d)\n",
        "chb=numpy.array(chb)\n",
        "rs=[]\n",
        "for l in range (2025,5763):\n",
        "  if((y_pred_all[l][0]==1) and (y_pred_all[l+1][0]==1) and (y_pred_all[l+2][0]==1)):\n",
        "    rs.append(l)\n",
        "    continue\n",
        "  \n",
        "\n",
        "print(rs)\n",
        "k=5763-2264\n",
        "\n",
        "print(\"the duration before seizure in min is\", (k*2)/60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s8392wqoF7I"
      },
      "outputs": [],
      "source": [
        "dataset_output= pd.read_csv(\"./y_chb\"+ch+\"_15min.txt\",delimiter=\" \",header=None)\n",
        "dataset_output=numpy.array(dataset_output,float)\n",
        "dataset_output_new= dataset_output.reshape(dataset_output.shape[0], )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "admghtBL_0pr"
      },
      "outputs": [],
      "source": [
        "print(dataset_output_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZyQ5sCqoKhd"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_train_test, dataset_output_new, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJV_HGlZukg3"
      },
      "outputs": [],
      "source": [
        "x_train_new=x_train\n",
        "x_test_new=x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4umbBC1-xoi"
      },
      "outputs": [],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJq3wqVZogBe"
      },
      "outputs": [],
      "source": [
        "Y_train_new = tensorflow.keras.utils.to_categorical( y_train)\n",
        "Y_test_new = tensorflow.keras.utils.to_categorical(y_test)\n",
        "print(Y_train_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuEvdQ2-JgeF"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=3,\n",
        "                 activation='relu',data_format=\"channels_last\",padding='same',\n",
        "                 input_shape=(7,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(64, 3, activation='relu',padding='same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(128, 3, activation='relu',padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=2,strides=True))\n",
        "model.add(Dropout(0.3))#0.1\n",
        "model.add(Conv1D(512, 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
        "model.add(Dropout(0.4))#0.1\n",
        "\n",
        "model.add(Flatten()) # Flatten is the input layer of the Fully Connected\n",
        "model.add(Dense(100, activation='relu')) # gets input size from flatten\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))#0.2\n",
        "model.add(Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uh6dHbKmonA"
      },
      "outputs": [],
      "source": [
        "checkpointer = ModelCheckpoint(filepath='./'+'_best_weights_1h.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
        "\n",
        "history =model.fit(x_train_new, Y_train_new, epochs=100, batch_size=512, verbose=1, validation_split=0.15, callbacks=[checkpointer])\n",
        "\n",
        "model.save('weights_cnn2_t1_1h.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-wjq8rbmtHU"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "import matplotlib.pyplot as plt # shorcut for a plot function\n",
        "plt.plot(history.history['accuracy']) # training accuracy\n",
        "plt.plot(history.history['val_accuracy']) # validation accuracy\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "predictions_1h = model.predict(x_test_new, batch_size=32, verbose=2)\n",
        "\n",
        "print (\"predicted images size :\",predictions_1h.shape)\n",
        "print(predictions_1h)\n",
        "print(Y_test_new.shape)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score\n",
        "threshold_confusion = 0.5\n",
        "print(y_test)\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "y_pred_1h = np.empty((predictions_1h.shape[0]))\n",
        "y_test = np.empty((predictions_1h.shape[0]))\n",
        "for i in range(predictions_1h.shape[0]):\n",
        "    \n",
        "    y_pred_1h[i]=np.argmax(predictions_1h[i])\n",
        "    y_test[i]=np.argmax(Y_test_new[i])\n",
        "print(y_pred_1h)\n",
        "\"\"\"  \n",
        "for i in range (0, len(y_pred_1h)):\n",
        "  if (y_pred_1h[i]==0):\n",
        "    y_pred_1h[i]=1\n",
        "  else: \n",
        "    y_pred_1h[i]=0\n",
        "\"\"\"\n",
        "print(y_test)\n",
        "confusion = confusion_matrix(y_test,  y_pred_1h)\n",
        "print (confusion)\n",
        "numpy.savetxt(\"./y_pred_1h.txt\",y_pred_1h,delimiter=\" \",fmt='%f')\n",
        "print(\"The accuracy score on this random test-set is  :\", accuracy_score(y_test,  y_pred_1h) )\n",
        "\n",
        "recall=recall_score(y_test,  y_pred_1h, average=None)\n",
        "av_recall=sum(recall)/2\n",
        "print(\"recall\",recall, \"the avreage recall is \",av_recall)\n",
        "precision=precision_score(y_test,  y_pred_1h,average=None)\n",
        "av_precision=sum(precision)/2\n",
        "\n",
        "print(\"precision\",precision,\"the avreage precision is \",av_precision)\n",
        "\n",
        "F1_score=(2*av_precision*av_recall)/(av_precision+av_recall)\n",
        "print(\"F1_score\",F1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbnjq3Gu0My_"
      },
      "outputs": [],
      "source": [
        "predictions_all = model.predict(x_train_test, batch_size=32, verbose=2)\n",
        "print (\"predicted images size :\",predictions_all.shape)\n",
        "print(predictions_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Z3Bq9H1lgk"
      },
      "outputs": [],
      "source": [
        "y_pred_all=predictions_all\n",
        "\n",
        "for i in range(predictions_all.shape[0]):\n",
        "    \n",
        "    y_pred_all[i]=np.argmax(predictions_all[i])\n",
        "print(y_pred_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKMghsRJ1wmq"
      },
      "outputs": [],
      "source": [
        "z=[]\n",
        "for i in range(len(y_pred_all)):\n",
        "    if(y_pred_all[i][0]==1):\n",
        "        z.append(i)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiWP6U__H-2k"
      },
      "outputs": [],
      "source": [
        "predictions_all = model.predict(x_train_test, batch_size=32, verbose=2)\n",
        "print (\"predicted images size :\",predictions_all.shape)\n",
        "print(predictions_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNwc58Pe25yo"
      },
      "outputs": [],
      "source": [
        "d=[67,2025]\n",
        "chb=[3297, 6265, 9859, 13530]\n",
        "d=numpy.array(d)\n",
        "chb=numpy.array(chb)\n",
        "rs=[]\n",
        "for l in range (67,1864):\n",
        "  if((y_pred_all[l][0]==1) and (y_pred_all[l+1][0]==1) and (y_pred_all[l+2][0]==1)):\n",
        "    rs.append(l)\n",
        "    continue\n",
        "  \n",
        "\n",
        "print(rs)\n",
        "k=1864-68\n",
        "\n",
        "print(\"the duration before seizure in min is\", (k*2)/60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Classification_crises_epilepsie.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}